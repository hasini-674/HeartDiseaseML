{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d66b1bd-5bb6-4eb1-87d8-ae67a76ef759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ae8bbc-0cf5-4a88-b34d-d1c838cb7f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4141037f-fd97-4c64-ad1c-645a3a5b67bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from xgboost) (1.11.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e13b5f-6b65-40b2-835b-335504990394",
   "metadata": {},
   "source": [
    "# Guassian Navie Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb19638-3e69-4f4b-8a6c-c8b1588f1949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.594\n",
      "Precision: 0.7140554480980013\n",
      "Recall: 0.31588705077010837\n",
      "F1 Score: 0.438006723353767\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.87      0.68      6988\n",
      "           1       0.71      0.32      0.44      7012\n",
      "\n",
      "    accuracy                           0.59     14000\n",
      "   macro avg       0.64      0.59      0.56     14000\n",
      "weighted avg       0.64      0.59      0.56     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/Users/tadisinahasini/Downloads/cardio_train.csv',delimiter=';')\n",
    "\n",
    "# Drop unnecessary columns like 'id'\n",
    "data = data.drop('id', axis=1)\n",
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = data.drop('cardio', axis=1)  # Features\n",
    "y = data['cardio']  # Target variable\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957648ef-60e1-4bec-957f-1b6de1d5629a",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233d7891-42fa-46c6-a89a-72291f2e0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Confusion Matrix:\n",
      "[[35 14]\n",
      " [14 37]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71        49\n",
      "           1       0.73      0.73      0.73        51\n",
      "\n",
      "    accuracy                           0.72       100\n",
      "   macro avg       0.72      0.72      0.72       100\n",
      "weighted avg       0.72      0.72      0.72       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load dataset with correct delimiter\n",
    "df = pd.read_csv('/Users/tadisinahasini/Downloads/cardio_train.csv', delimiter=';')\n",
    "\n",
    "# Ensure the target column is named correctly\n",
    "target_column = 'cardio'\n",
    "\n",
    "# Drop 'id' column as it is not needed for the analysis\n",
    "df = df.drop(columns=['id'])\n",
    "df = df.sample(n=200, random_state=42)\n",
    "# Convert categorical variables like 'gender' into dummy variables if needed\n",
    "# Example: df = pd.get_dummies(df, columns=['gender'])\n",
    "\n",
    "# Split dataset into features (X) and target (y)\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Print confusion matrix and classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6b37c-49b9-48e8-896a-835ae321df09",
   "metadata": {},
   "source": [
    "# Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76068790-91c1-4b34-81cc-e8da5e507c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7246857142857143\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.81      0.75     17430\n",
      "           1       0.77      0.64      0.70     17570\n",
      "\n",
      "    accuracy                           0.72     35000\n",
      "   macro avg       0.73      0.73      0.72     35000\n",
      "weighted avg       0.73      0.72      0.72     35000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/Users/tadisinahasini/Downloads/cardio_train.csv',delimiter=';')\n",
    "\n",
    "# Drop unnecessary columns like 'id'\n",
    "data = data.drop('id', axis=1)\n",
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = data.drop('cardio', axis=1)  # Features\n",
    "y = data['cardio']  # Target variable\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Support Vector Machine classifier\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567651cf-ab70-4b7e-b592-9f5f291e9fab",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4704325-3d61-42fd-a1d3-494a111a500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h4/1q915gtd17x3_f6dd0gqm51c0000gn/T/ipykernel_1394/1682395437.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.60%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63      6988\n",
      "           1       0.63      0.61      0.62      7012\n",
      "\n",
      "    accuracy                           0.63     14000\n",
      "   macro avg       0.63      0.63      0.63     14000\n",
      "weighted avg       0.63      0.63      0.63     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load Data\n",
    "data = pd.read_csv('/Users/tadisinahasini/Downloads/cardio_train.csv',delimiter=';')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Handling missing values (if any)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Separating features and target variable\n",
    "X = data.drop('cardio', axis=1)  # Features\n",
    "y = data['cardio']  # Target variable\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Implement k-NN Algorithm\n",
    "# Choosing the value of k\n",
    "k = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Training the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 6: Optimize k (optional, can be done in a loop with cross-validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689e776-eebc-42bd-a8e6-733c45794e8c",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d780298a-4eba-469a-847b-e15567d7c3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h4/1q915gtd17x3_f6dd0gqm51c0000gn/T/ipykernel_1394/926571243.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.72%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73      6988\n",
      "           1       0.74      0.70      0.72      7012\n",
      "\n",
      "    accuracy                           0.73     14000\n",
      "   macro avg       0.73      0.73      0.73     14000\n",
      "weighted avg       0.73      0.73      0.73     14000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5265 1723]\n",
      " [2096 4916]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load Data\n",
    "data = pd.read_csv('/Users/tadisinahasini/Downloads/cardio_train.csv',delimiter=';')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Handling missing values (if any)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Separating features and target variable\n",
    "X = data.drop('cardio', axis=1)  # Features\n",
    "y = data['cardio']  # Target variable\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Implement Random Forest Algorithm\n",
    "# Initializing the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78312d9-1ae5-4920-bcd9-393c62896c46",
   "metadata": {},
   "source": [
    "# Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c647490-98b0-4b23-a2ea-d5402104ffb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.80%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75      6988\n",
      "           1       0.76      0.70      0.73      7012\n",
      "\n",
      "    accuracy                           0.74     14000\n",
      "   macro avg       0.74      0.74      0.74     14000\n",
      "weighted avg       0.74      0.74      0.74     14000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h4/1q915gtd17x3_f6dd0gqm51c0000gn/T/ipykernel_1681/2646370109.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "data = pd.read_csv('/Users/tadisinahasini/Downloads/cardio_train.csv',delimiter=';')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Handling missing values (if any)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Separating features and target variable\n",
    "X = data.drop('cardio', axis=1)  # Features\n",
    "y = data['cardio']  # Target variable\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Assuming data is preprocessed and available as X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing the XGBoost Classifier\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc96ad-34a7-4272-af4e-43b3e647bc98",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16fc024f-0fce-4935-bb1e-ae9f86ffd17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h4/1q915gtd17x3_f6dd0gqm51c0000gn/T/ipykernel_22152/1969334251.py:14: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400/1400 [==============================] - 1s 627us/step - loss: 0.5966 - accuracy: 0.6898 - val_loss: 0.5681 - val_accuracy: 0.7187\n",
      "Epoch 2/20\n",
      "1400/1400 [==============================] - 1s 570us/step - loss: 0.5652 - accuracy: 0.7228 - val_loss: 0.5590 - val_accuracy: 0.7268\n",
      "Epoch 3/20\n",
      "1400/1400 [==============================] - 1s 578us/step - loss: 0.5576 - accuracy: 0.7273 - val_loss: 0.5532 - val_accuracy: 0.7328\n",
      "Epoch 4/20\n",
      "1400/1400 [==============================] - 1s 638us/step - loss: 0.5494 - accuracy: 0.7293 - val_loss: 0.5475 - val_accuracy: 0.7345\n",
      "Epoch 5/20\n",
      "1400/1400 [==============================] - 1s 573us/step - loss: 0.5459 - accuracy: 0.7318 - val_loss: 0.5463 - val_accuracy: 0.7301\n",
      "Epoch 6/20\n",
      "1400/1400 [==============================] - 1s 572us/step - loss: 0.5452 - accuracy: 0.7322 - val_loss: 0.5434 - val_accuracy: 0.7346\n",
      "Epoch 7/20\n",
      "1400/1400 [==============================] - 1s 571us/step - loss: 0.5422 - accuracy: 0.7341 - val_loss: 0.5424 - val_accuracy: 0.7350\n",
      "Epoch 8/20\n",
      "1400/1400 [==============================] - 1s 572us/step - loss: 0.5426 - accuracy: 0.7346 - val_loss: 0.5431 - val_accuracy: 0.7359\n",
      "Epoch 9/20\n",
      "1400/1400 [==============================] - 1s 571us/step - loss: 0.5407 - accuracy: 0.7348 - val_loss: 0.5482 - val_accuracy: 0.7329\n",
      "Epoch 10/20\n",
      "1400/1400 [==============================] - 1s 571us/step - loss: 0.5402 - accuracy: 0.7342 - val_loss: 0.5457 - val_accuracy: 0.7337\n",
      "Epoch 11/20\n",
      "1400/1400 [==============================] - 1s 572us/step - loss: 0.5404 - accuracy: 0.7355 - val_loss: 0.5445 - val_accuracy: 0.7323\n",
      "Epoch 12/20\n",
      "1400/1400 [==============================] - 1s 573us/step - loss: 0.5396 - accuracy: 0.7344 - val_loss: 0.5436 - val_accuracy: 0.7333\n",
      "Epoch 13/20\n",
      "1400/1400 [==============================] - 1s 572us/step - loss: 0.5385 - accuracy: 0.7356 - val_loss: 0.5452 - val_accuracy: 0.7323\n",
      "Epoch 14/20\n",
      "1400/1400 [==============================] - 1s 572us/step - loss: 0.5379 - accuracy: 0.7353 - val_loss: 0.5439 - val_accuracy: 0.7332\n",
      "Epoch 15/20\n",
      "1400/1400 [==============================] - 1s 571us/step - loss: 0.5375 - accuracy: 0.7366 - val_loss: 0.5462 - val_accuracy: 0.7321\n",
      "Epoch 16/20\n",
      "1400/1400 [==============================] - 1s 572us/step - loss: 0.5368 - accuracy: 0.7374 - val_loss: 0.5419 - val_accuracy: 0.7332\n",
      "Epoch 17/20\n",
      "1400/1400 [==============================] - 1s 600us/step - loss: 0.5361 - accuracy: 0.7363 - val_loss: 0.5428 - val_accuracy: 0.7361\n",
      "Epoch 18/20\n",
      "1400/1400 [==============================] - 1s 571us/step - loss: 0.5355 - accuracy: 0.7374 - val_loss: 0.5464 - val_accuracy: 0.7308\n",
      "Epoch 19/20\n",
      "1400/1400 [==============================] - 1s 572us/step - loss: 0.5348 - accuracy: 0.7387 - val_loss: 0.5465 - val_accuracy: 0.7329\n",
      "Epoch 20/20\n",
      "1400/1400 [==============================] - 1s 571us/step - loss: 0.5352 - accuracy: 0.7374 - val_loss: 0.5437 - val_accuracy: 0.7340\n",
      "438/438 [==============================] - 0s 308us/step\n",
      "Accuracy: 73.91%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75      6988\n",
      "           1       0.75      0.71      0.73      7012\n",
      "\n",
      "    accuracy                           0.74     14000\n",
      "   macro avg       0.74      0.74      0.74     14000\n",
      "weighted avg       0.74      0.74      0.74     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "data = pd.read_csv('/Users/tadisinahasini/Downloads/cardio_train.csv',delimiter=';')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Handling missing values (if any)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Separating features and target variable\n",
    "X = data.drop('cardio', axis=1)  # Features\n",
    "y = data['cardio']  # Target variable\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Assuming data is preprocessed and available as X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30830d-2a6c-4ff3-a9c7-f188cb16ff9d",
   "metadata": {},
   "source": [
    "# Ensembling Different Models\n",
    "### This model aims to predict the presence of heart disease by ensembling various machine learning algorithms. The models used for prediction include:\n",
    "\n",
    " - Gaussian Naive Bayes\n",
    " - Logistic Regression\n",
    " - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ab5fc6-9095-4699-8b8c-fbd044cc99ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h4/1q915gtd17x3_f6dd0gqm51c0000gn/T/ipykernel_1983/2033937607.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.15%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73      6988\n",
      "           1       0.74      0.68      0.71      7012\n",
      "\n",
      "    accuracy                           0.72     14000\n",
      "   macro avg       0.72      0.72      0.72     14000\n",
      "weighted avg       0.72      0.72      0.72     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('/Users/tadisinahasini/Downloads/cardio_train.csv',delimiter=';')\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "X = data.drop('cardio', axis=1)\n",
    "y = data['cardio']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize base models\n",
    "nb = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Create a function for stacking\n",
    "def get_stacking(models, meta_model, X_train, y_train, X_test, y_test, n_folds=5):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Train base models\n",
    "    S_train = np.zeros((X_train.shape[0], len(models)))\n",
    "    S_test = np.zeros((X_test.shape[0], len(models)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        S_test_i = np.zeros((X_test.shape[0], n_folds))\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):\n",
    "            X_fold_train, y_fold_train = X_train[train_idx], y_train.iloc[train_idx]\n",
    "            X_fold_valid, y_fold_valid = X_train[valid_idx], y_train.iloc[valid_idx]\n",
    "            \n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            S_train[valid_idx, i] = model.predict(X_fold_valid)\n",
    "            S_test_i[:, fold] = model.predict(X_test)\n",
    "        \n",
    "        S_test[:, i] = S_test_i.mean(axis=1)\n",
    "    \n",
    "    # Train meta-model\n",
    "    meta_model.fit(S_train, y_train)\n",
    "    y_pred = meta_model.predict(S_test)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# List of base models\n",
    "models = [nb, lr]\n",
    "\n",
    "# Meta-model\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Get stacking predictions\n",
    "y_pred = get_stacking(models, meta_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba197dec-e489-408d-b635-075fa8eb633a",
   "metadata": {},
   "source": [
    "### The models used in this ensemble model for prediction include:\n",
    "\n",
    " - Gaussian Naive Bayes\n",
    " - Logistic Regression\n",
    " - Gradient Boosting\n",
    " - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab41c56-1285-43a3-97c5-4e3fb6e2d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h4/1q915gtd17x3_f6dd0gqm51c0000gn/T/ipykernel_22152/1784725972.py:14: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Accuracy: 73.73%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75     17430\n",
      "           1       0.76      0.70      0.73     17570\n",
      "\n",
      "    accuracy                           0.74     35000\n",
      "   macro avg       0.74      0.74      0.74     35000\n",
      "weighted avg       0.74      0.74      0.74     35000\n",
      "\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.7s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   2.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   2.6s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   2.6s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   2.4s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   2.6s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   2.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   2.3s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   1.5s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   2.4s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   2.6s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   1.3s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   0.8s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   2.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   2.6s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   1.2s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time=   2.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=   2.6s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=   2.4s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=   2.5s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=   1.5s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('/Users/tadisinahasini/Downloads/cardio_train.csv',delimiter=';')\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "X = data.drop('cardio', axis=1)\n",
    "y = data['cardio']\n",
    "\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
    "# Initialize base models\n",
    "nb = GaussianNB()\n",
    "lr = LogisticRegression(random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "\n",
    "# Create a function for stacking\n",
    "def get_stacking(models, meta_model, X_train, y_train, X_test, y_test, n_folds=5):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Train base models\n",
    "    S_train = np.zeros((X_train.shape[0], len(models)))\n",
    "    S_test = np.zeros((X_test.shape[0], len(models)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        S_test_i = np.zeros((X_test.shape[0], n_folds))\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):\n",
    "            X_fold_train, y_fold_train = X_train[train_idx], y_train.iloc[train_idx]\n",
    "            X_fold_valid, y_fold_valid = X_train[valid_idx], y_train.iloc[valid_idx]\n",
    "            \n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            S_train[valid_idx, i] = model.predict(X_fold_valid)\n",
    "            S_test_i[:, fold] = model.predict(X_test)\n",
    "        \n",
    "        S_test[:, i] = S_test_i.mean(axis=1)\n",
    "    \n",
    "    # Hyperparameter tuning for meta-model\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=meta_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(S_train, y_train)\n",
    "    \n",
    "    print(f'Best Parameters: {grid_search.best_params_}')\n",
    "    best_meta_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Train and predict with the best meta-model\n",
    "    best_meta_model.fit(S_train, y_train)\n",
    "    y_pred = best_meta_model.predict(S_test)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# List of base models\n",
    "models = [nb, lr, xgb]\n",
    "\n",
    "# Meta-model\n",
    "meta_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Get stacking predictions\n",
    "y_pred = get_stacking(models, meta_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2784206-c285-49b3-84f9-573aa5741d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
